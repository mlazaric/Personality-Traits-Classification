% Paper template for TAR 2021
% (C) 2014 Jan Šnajder, Goran Glavaš, Domagoj Alagić, Mladen Karan
% TakeLab, FER

\documentclass[10pt, a4paper]{article}

\usepackage{tar2021}

\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Syntax vs Semantics: What tells more about your personality?}

\name{Marko Lazarić, Laura Torić, Roman Yatsukha}

\address{
University of Zagreb, Faculty of Electrical Engineering and Computing\\
Unska 3, 10000 Zagreb, Croatia\\
\texttt{\{marko.lazaric,laura.toric,roman.yatsukha\}@fer.hr}\\
}

\abstract{
  abstract
}

\begin{document}

\maketitleabstract

\section{Introduction}
Motivation (What we write tells alot about who we are.. bla bla ... how far can we go...)
Big Five
Previous work (only uses sentiment)
But (what about the syntax?)
We (will see what is more important syntax or semantics..)
Results are showing..
Overview

\section{Previous work}
Something about the papers that we had to read for first chekpoint

\section{Features}
About dataset
POS, NE, embeddings, bag of words

The dataset used for this paper is from [[PENNEBAKER CITATION]]'s \cite{pennebaker} collection of summer school student essays. The students' personality traits were assessed using the BFI self-report questionaire [[BFI CITATION?]].

SpaCy\footnote{https://spacy.io} was used for lemmatization, part-of-speech tagging and named entity recognition.
The features extracted from the essays were bag of word features and the number of each part-of-speech and named entity tag in the essay.
To reduce the effect of the length of the essay on the features, alternative features were tried where each number was normalized by the sum of the part-of-speech or named entity tags to get the percentage of each tag or normalized by the number of sentences to get the mean number of each part-of-speech or named entity tag per sentence.

\section{Models}
SVM, decision trees, BERT, XLNet

\subsection{BERT}

Bidirectional Encoder Representations from Transformers, or just BERT, is a transformer based model for NLP tasks first presented in [[INSERT BERT CITATION HERE]]. BERT comes pretrained on two large corpora: BooksCorpus which consists of 800 million words, and the English Wikipedia with 2.5 billion words. The pretraining is done on two NLP taks: masked language modeling, where the model predicts a masked word in a sentence, and next sentence prediction. [[BERT CITATION]]

BERT requires fine tuning before it is ready for usage on other NLP tasks. For our use case we picked a promising personality trait [[NEED FURTHER EXPLANATION]], and fine-tuned a BERT\textsubscript{BASE}$-$cased variant of BERT which was pretrained on cased input, ie. "Mark" and "mark" are different tokens. The model has 110 million parameters to fine tune.

[[MORE ON DATASET?]] To transform essays into the desired input representation we used BertTokenizer from the HunggingFace library [[HUGGINGFAVCE CITATION]]. The tokenizer takes raw text and limits the input to 512 tokens so any essays longer than that are trimmed. The dataset was split into training and testing sets using 80:20, using shuffled batches of size 10 (batch size was limited due to the available GPU memory). Adam was used as the optimizer [[ADAM CITATION]] with a learning rate of $10^{-5}$ and no regularization.

[[RESULTS HERE: TODO RUN BERT AGAIN >.>]]

\section{Experiments}
1.\ best model and features for each personality
2.\ ttest for different pos and ne between different personalities
3.\ most significant words/pos tags/named entities for each personality

\section{Results}
Tables and graphs (and wordclouds)

\section{Conclusion}
Well this was fun.

\bibliographystyle{tar2021}
\bibliography{tar2021}

\end{document}
