% Paper template for TAR 2021
% (C) 2014 Jan Šnajder, Goran Glavaš, Domagoj Alagić, Mladen Karan
% TakeLab, FER

\documentclass[10pt, a4paper]{article}

\usepackage{tar2021}

\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}

\title{Syntax vs Semantics: What tells more about your personality?}

\name{Marko Lazarić, Laura Torić, Roman Yatsukha}

\address{
University of Zagreb, Faculty of Electrical Engineering and Computing\\
Unska 3, 10000 Zagreb, Croatia\\
\texttt{\{marko.lazaric,laura.toric,roman.yatsukha\}@fer.hr}\\
}

\abstract{
	Personality traits are inherently subjective and hard to define categories, so classifying peoples' personalities is a challenging task even for people.
	On the other hand, natural language processing has made great progress in similarly subjective areas such as sentiment analysis.
	In this paper, we compare two approaches to classifying individual personality traits based on essays: synctactic and semantic.
	The syntactic approach uses the role of the word in the sentence, such as subject, object, verb, etc., while the semantic approach uses the meaning of the words themselves to classify the essays.
	We compare the two approaches on a dataset of 2467 student essays annotated with the big five personality traits and show that different approaches are better suited for different personality traits.
}

\begin{document}

\maketitleabstract

\section{Introduction}

Every text contains a remarkable amount of information about the author.
While that information might not be apparent to people, natural language processing has shown itself very adept to using that information to classify texts and their authors' into certain categories.

An example of such a task is classifying the personality of the author based on their essay.
Since a personality is hard to define, it is usually decomposed into several personality traits which are usually binary.
The big five personality model is a popular personality trait model which will be used in this paper.
It uses the following five personality traits: extraversion, agreeableness, openness, conscientiousness and neuroticism.

In this paper, we experiment with two approaches to classifying the different personality traits: syntactic and semantic.
The syntactic approach uses the role of the words in the sentence such as subject, object, etc. without knowing what the words mean.
The semantic approach uses the meanings of the words, either through learned embeddings or the bag of words model.
Our results show that different approaches work better for different personality traits.
%Motivation (What we write tells alot about who we are.. bla bla ... how far can we go...)
%Big Five
%Previous work (only uses sentiment)
%But (what about the syntax?)
%We (will see what is more important syntax or semantics..)
%Results are showing..
%Overview

\section{Previous work}
Something about the papers that we had to read for first chekpoint

\section{Features}
%About dataset
%POS, NE, embeddings, bag of words

The dataset used for this paper is from \cite{pennebaker}, a collection of student essays.
The students' personality traits were assessed using the big five inventory (BFI) self-report questionnaire.
The big five personality traits that were assessed are: extraversion, agreeableness, openness, conscientiousness and neuroticism.
The dataset contains 2467  essays with a mostly balanced number of positive and negative examples for each class which is shown in table \ref{positive negative examples}.

\begin{table}[H]
	\begin{tabular}{lrr}
		\hline
		Personality trait & Negative & Positive \\ \hline
		Extraversion                                  & 1191 (48.28\%)                         & 1276 (51.72\%)                         \\
		Neuroticism                                  & 1234 (50.02\%)                         & 1233 (49.98\%)                         \\
		Agreeableness                                  & 1157 (46.90\%)                         & 1310 (53.10\%)                         \\
		Conscientiousness                                  & 1214 (49.21\%)                         & 1253 (50.79\%)                         \\
		Openness                                  & 1196 (48.48\%)                         & 1271 (51.52\%)                         \\ \hline
	\end{tabular}
	\caption{The number of positive and negative examples for each personality trait.}
	\label{positive negative examples}
\end{table}

Our goal is to correctly classify all of the big five personality traits based on the given essay.

The features extracted from the essays were bag-of-words features and the number of each part-of-speech and named entity tag in the essay.
SpaCy\footnote{https://spacy.io} was used for lemmatization, part-of-speech tagging and named entity recognition.
To reduce the effect of the length of the essay on the features, alternative features were tried where each number was normalized by the sum of the part-of-speech or named entity tags to get the percentage of each tag or normalized by the number of sentences to get the mean number of each part-of-speech or named entity tag per sentence.

For the semantic models we used word embeddings.

\section{Models}
%SVM, decision trees, BERT, XLNet

Since the different personality traits are independent, a separate model was trained to classify each personality trait.
To test the predictive power of syntactic features, SVMs were used, while BERT and XLNet were used as representative semantic models.
The individual models and their pipelines are further explained in the following subsections.

\subsection{SVM}

Support Vector Machines from scikit-learn\footnote{https://scikit-learn.org/} were trained to classify the essays for each personality trait.
Feature selection was done by selecting a certain number of features with the highest ANOVA F-value.
After that, the selected features were standardized by removing the mean and scaling to unit variance and used to train the SVMs.

\subsection{BERT}

Bidirectional Encoder Representations from Transformers, or just BERT, is a transformer based model for NLP tasks first presented in [[INSERT BERT CITATION HERE]]. BERT comes pretrained on two large corpora: BooksCorpus which consists of 800 million words, and the English Wikipedia with 2.5 billion words. The pretraining is done on two NLP taks: masked language modeling, where the model predicts a masked word in a sentence, and next sentence prediction. [[BERT CITATION]]

BERT requires fine tuning before it is ready for usage on other NLP tasks. For our use case we picked a promising personality trait [[NEED FURTHER EXPLANATION]], and fine-tuned a BERT\textsubscript{BASE}$-$cased variant of BERT which was pretrained on cased input, ie. "Mark" and "mark" are different tokens. The model has 110 million parameters to fine tune.

[[MORE ON DATASET?]] To transform essays into the desired input representation we used BertTokenizer from the HunggingFace library [[HUGGINGFAVCE CITATION]]. The tokenizer takes raw text and limits the input to 512 tokens so any essays longer than that are trimmed. The dataset was split into training and testing sets using 80:20, using shuffled batches of size 10 (batch size was limited due to the available GPU memory). Adam was used as the optimizer [[ADAM CITATION]] with a learning rate of $10^{-5}$ and no regularization.

[[RESULTS HERE: TODO RUN BERT AGAIN >.>]]

\section{Experiments}
To test our syntax and semantic models we conducted three experiments described in this section.

\textbf{Experiment 1.} To test the performance of different models and features we decided to compare F1 and accuracy scores measured on the test set.
For the syntax models we used SVM with all possible combinations of part-of-speech and named entity tags and tested all of them.
For the semantic models we used BERT, XLNet and SVM with bag-of-words features.

\textbf{Experiment 2.}  In order to test if some personality traits had different distributions of part-of-speech or named entity tags than others, we used a t-test, or more accurately, Welch’s t-test.
For a few selected most promising tags we tested the difference between every pair of traits and reported p values.
If the p value was less than 0.05 the difference was considered significant.

\textbf{Experiment 3.} For our last experiment we wanted to find the most descriptive features for each trait using a univariate f test.
Using only those features we tested to see if there is any difference in a model's performance.

\section{Results}
Tables and graphs (and wordclouds)

\section{Conclusion}
Well this was fun.

\bibliographystyle{tar2021}
\bibliography{tar2021}

\end{document}
