{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TAR_XLNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJG7ssGUxTlh"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGBlj03wzTqu",
        "outputId": "12fcbcf3-56e1-4d3b-e431-d8f98c53092e"
      },
      "source": [
        "from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from collections import defaultdict\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "MAX_LEN = 500\n",
        "EPOCHS = 5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LdCiS_awlbR"
      },
      "source": [
        "class EssaysDataset(Dataset):\n",
        "    def __init__(self, texts, target, max_len, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.target = target\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        target = self.target[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(text,\n",
        "                                              add_special_tokens=True,\n",
        "                                              max_length=self.max_len,\n",
        "                                              truncation=True,\n",
        "                                              return_token_type_ids=False,\n",
        "                                              pad_to_max_length=False,\n",
        "                                              return_attention_mask=True,\n",
        "                                              return_tensors='pt')\n",
        "\n",
        "        input_ids = pad_sequences(encoding['input_ids'],\n",
        "                                  dtype=torch.Tensor,\n",
        "                                  maxlen=self.max_len,\n",
        "                                  truncating=\"post\",\n",
        "                                  padding=\"post\")\n",
        "        input_ids = input_ids.astype(dtype='int64')\n",
        "        input_ids = torch.tensor(input_ids)\n",
        "\n",
        "        attention_mask = pad_sequences(encoding['attention_mask'],\n",
        "                                       dtype=torch.Tensor,\n",
        "                                       maxlen=self.max_len,\n",
        "                                       truncating=\"post\",\n",
        "                                       padding=\"post\")\n",
        "        attention_mask = attention_mask.astype(dtype='int64')\n",
        "        attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "        return {'text': text,\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': attention_mask.flatten(),\n",
        "                'target': torch.tensor(target, dtype=torch.long)}\n",
        "\n",
        "\n",
        "def create_data_loader(df, target, tokenizer, max_len, batch_size):\n",
        "    ds = EssaysDataset(texts=df['TEXT'].to_numpy(),\n",
        "                       target=df[target].to_numpy(),\n",
        "                       max_len=max_len,\n",
        "                       tokenizer=tokenizer)\n",
        "\n",
        "    return DataLoader(ds, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n",
        "    model = model.train()\n",
        "    acc, losses = [], []\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].reshape((-1, MAX_LEN)).to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        target = d[\"target\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels=target)\n",
        "        loss, logits = outputs.loss, outputs.logits\n",
        "\n",
        "        _, prediction = torch.max(logits, dim=1)\n",
        "        target = target.cpu().detach().numpy()\n",
        "        prediction = prediction.cpu().detach().numpy()\n",
        "\n",
        "        accuracy = accuracy_score(target, prediction)\n",
        "        acc.append(accuracy)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.5)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return np.mean(acc), np.mean(losses)\n",
        "\n",
        "\n",
        "def eval_model(model, data_loader, device, n_examples):\n",
        "    model = model.eval()\n",
        "    acc, losses, f1 = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].reshape((-1, MAX_LEN)).to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            target = d[\"target\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, token_type_ids=None, attention_mask=attention_mask, labels=target)\n",
        "            loss, logits = outputs.loss, outputs.logits\n",
        "\n",
        "            _, prediction = torch.max(logits, dim=1)\n",
        "            target = target.cpu().detach().numpy()\n",
        "            prediction = prediction.cpu().detach().numpy()\n",
        "\n",
        "            accuracy = accuracy_score(target, prediction)\n",
        "            p, r, f, s = precision_recall_fscore_support(target, prediction, average='binary')\n",
        "            acc.append(accuracy)\n",
        "            f1.append(f)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return np.mean(acc), np.mean(losses), np.mean(f1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBoddGMTzbDc"
      },
      "source": [
        "# AUTHID,TEXT,cEXT,cNEU,cAGR,cCON,cOPN\n",
        "df = pd.read_csv('datasets/essays.csv', encoding='latin-1')\n",
        "\n",
        "traits = ['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']\n",
        "transformation = {'n': 0, 'y': 1}\n",
        "df = df.replace({'cEXT': transformation,\n",
        "                 'cNEU': transformation,\n",
        "                 'cAGR': transformation,\n",
        "                 'cCON': transformation,\n",
        "                 'cOPN': transformation})\n",
        "\n",
        "\n",
        "df_train = df.sample(frac=0.9)\n",
        "df_test = df.drop(df_train.index)\n",
        "df_val = df_train.sample(frac=0.1)\n",
        "df_train = df_train.drop(df_val.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDehWQLdzmeC"
      },
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, traits[4], tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, traits[4], tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, traits[4], tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxHy5QchzLhS",
        "outputId": "9bc94888-6035-4cd2-b0a1-7d4938c92d45"
      },
      "source": [
        "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\n",
        "model = model.to(device)\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "                                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n",
        "                               ]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
        "\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=total_steps)\n",
        "\n",
        "history = defaultdict(list)\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    train_acc, train_loss = train_epoch(model,\n",
        "                                        train_data_loader,\n",
        "                                        optimizer,\n",
        "                                        device,\n",
        "                                        scheduler,\n",
        "                                        len(df_train))\n",
        "    print(f'Train loss {train_loss} Train accuracy {train_acc}')\n",
        "\n",
        "    val_acc, val_loss, val_f1 = eval_model(model,\n",
        "                                           val_data_loader,\n",
        "                                           device,\n",
        "                                           len(df_val))\n",
        "    print(f'Valid loss {val_loss:.4f} Valid accuracy {val_acc:.4f} Valid F1 {val_f1:.4f}')\n",
        "    print()\n",
        "\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "\n",
        "test_acc, test_loss, test_f1 = eval_model(model,\n",
        "                                          test_data_loader,\n",
        "                                          device,\n",
        "                                          len(df_test))\n",
        "print(f'Test loss {test_loss:.4f} Test accuracy {test_acc:.4f} Test F1 {test_f1:.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "----------\n",
            "Train loss 0.685509972423315 Train accuracy 0.5631666666666666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Valid loss 0.6915 Valid accuracy 0.5733 Valid F1 0.2096\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "Train loss 0.6552249873802066 Train accuracy 0.6116666666666667\n",
            "Valid loss 0.7199 Valid accuracy 0.6311 Valid F1 0.3638\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "Train loss 0.6046022529155016 Train accuracy 0.6771666666666667\n",
            "Valid loss 0.8549 Valid accuracy 0.6044 Valid F1 0.3527\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "Train loss 0.5216176130063832 Train accuracy 0.7501666666666666\n",
            "Valid loss 1.2500 Valid accuracy 0.5956 Valid F1 0.3423\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "Train loss 0.46042319727595893 Train accuracy 0.8008333333333333\n",
            "Valid loss 1.1861 Valid accuracy 0.5933 Valid F1 0.4589\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss 1.0094 Test accuracy 0.6360 Test F1 0.4882\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}